# LLM txt 

## Dataset

Make sure to prepare the dataset for the QA based llm chat is prepared in the .txt format 

## Environment

### creating environment 
python -m venv myenv

### Activating Environment
myenv/Scripts/Activate

### Dependencies
install all the libraries mentioned in the requirements.txt file by running this code
pip install -r requirements.txt

### Downloading Local llama Model
you can download local llama model from this Huggindface repo **TheBloke/Llama-2-7B-Chat-GGML**  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML
make sure to add the downloaded model into the work space

### Running the main code
 open the terminal and type python app.py (replace it with whatever your file name is)

 Now your model wulll be up and running in the loacl host

 ### frontend
 open your frontend code a start it with the live server in vs code

 ## Now the bot is ready to asnwer your questions

 
